{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHx2_ZGJQiKZ",
        "outputId": "7f72f161-4544-4991-e3ba-802176c0c287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.1)\n",
            "Requirement already satisfied: openpyxl in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.2)\n",
            "Requirement already satisfied: requests in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lordm\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.3)\n",
            "Requirement already satisfied: et-xmlfile in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\lordm\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: wget in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: gdown in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.7.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in c:\\users\\lordm\\appdata\\roaming\\python\\python312\\site-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\lordm\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->gdown) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: unidecode in c:\\users\\lordm\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.7)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Instalando las paqueterías necesarias.\n",
        "%pip install pandas openpyxl requests beautifulsoup4\n",
        "%pip install wget\n",
        "%pip install gdown\n",
        "%pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlT8D8oM1f2p"
      },
      "source": [
        "Las siguientes librerías nos ayudaran para el desarrollo del proceso de descarga y generación del archivo txt. es necesario que este al principio para su correcto funcionamiento.\n",
        "\n",
        "Se utiliza librerías especiales como gwdown para descarga de archivos grandes, eliminando la confirmación de permiso de descarga.\n",
        "\n",
        "Se utiliza librería unicode, para poder quitar los acentos de ciertas palabras y usarlo como nombre de archivos a descargar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gY6pHKnxVf02"
      },
      "outputs": [],
      "source": [
        "# Importación de librerías para usarlas en el proyecto de descarga y generación del txt con la descripción de esta\n",
        "import pandas as pd\n",
        "import wget\n",
        "import gdown\n",
        "import requests\n",
        "import re\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from logging import exception\n",
        "from unidecode import unidecode\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrKBE0zm2OZK"
      },
      "source": [
        "La inicialización de ciertas variables ayudaran a preparar la información inicial de cada una de ellas, además de que puedan ser usadas en cualquier parte del código o proceso, sin en el que perjudique el ámbito o espacio donde estas estén siendo seteadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "vB3CEL3HoRBp"
      },
      "outputs": [],
      "source": [
        "# Inicializando variables que serán usadas más adelante\n",
        "\n",
        "incidencia_delictiva_link = 'https://www.gob.mx/sesnsp/acciones-y-programas/datos-abiertos-de-incidencia-delictiva'\n",
        "incidencia_title = ''\n",
        "incidencia_description = ''\n",
        "\n",
        "folder_path_data = '../rawdata'\n",
        "folder_path_log = '../logs/descarga/'\n",
        "folder_path_error_log = '../logs/errores/'\n",
        "csv_filepath = ''\n",
        "\n",
        "# Creando variable que contendra el listado de cada source descargado.\n",
        "sources_list = []\n",
        "\n",
        "# Creando variable que contendra el listado de cada nombre de archivo descargado y que esta siendo enviado a nuestro repositorio\n",
        "names_files_list = []\n",
        "\n",
        "texto_list = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sección donde se encuentran las funciones generales del proceso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_error_write(description, line):\n",
        "    # Verificando si la carpeta existe, si no, crearla\n",
        "    if not os.path.exists(folder_path_error_log):\n",
        "        os.makedirs(folder_path_error_log)\n",
        "        \n",
        "    date_now = datetime.now()\n",
        "\n",
        "    # Variable que contiene el texto que tendrá el archivo txt que se generará y descargará.\n",
        "    line_error = f\"\"\"{date_now.strftime('%Y-%m-%d %H:%M:%S')} {line} - {description}\"\"\"\n",
        "    # Se crea el archivo a partir del texto de arriba, el cual se guardará con el nombre descriptivo más la fecha día*mes*año, para identificar la descarga por día.\n",
        "    with open(f'{folder_path_error_log}/ddf_err{date_now.strftime(\"%d%m%y\")}.txt', 'a', encoding=\"utf-8\") as error_file:\n",
        "        error_file.write(f\"\\n{line_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sección donde se encuentran las funciones para los distintos metodos de descarga"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "elWoM2gu1znm"
      },
      "outputs": [],
      "source": [
        "# sección de funciones para descarga de archivos desde fuentes que vienen de enlaces proporcionados por la página del gobierno.\n",
        "\n",
        "# Función utilizada para la descarga de archivos almacenados en google drive por parte de la institución donde se requiere obtener los datos.\n",
        "# - document: recibe el id del documento en google drive.\n",
        "# - description: Texto de la etiqueta de enlace, el cual describe la naturaleza de la información a descargar.\n",
        "def download_csv_from_http(document, description):\n",
        "\n",
        "  print(f\"link \" + description)\n",
        "\n",
        "  try:\n",
        "    # Verificando si la carpeta existe, si no, crearla\n",
        "    if not os.path.exists(folder_path_data):\n",
        "        os.makedirs(folder_path_data)\n",
        "\n",
        "    # Url de descarga a traves de google drive\n",
        "    csv_url = f'https://drive.google.com/uc?id={document}'\n",
        "\n",
        "    # Descargando archivos desde la url proporcionada\n",
        "    response_csv = requests.get(csv_url)\n",
        "\n",
        "    # Parte donde se hace una limpieza del parámetro description enviado,\n",
        "    # esto para construir el nombre que tendrá el archivo csv que se descarga a partir de los datos de las fuentes\n",
        "    # Utilizando expresión regular para reemplazar caracteres especiales y espacios por un solo guion bajo aplicándolo en la descripción del enlace.\n",
        "    csv_filename = re.sub(r'[^\\w\\s]+', '', description)\n",
        "    csv_filename = re.sub(r'\\s+', '', csv_filename)\n",
        "    # Utilizando unidecode para quitar los acentos y no cause conflictos como nombre de archivo\n",
        "    csv_filename = unidecode(csv_filename)\n",
        "    csv_filename = csv_filename + \".csv\"\n",
        "\n",
        "    # Construyendo la ruta completa del archivo\n",
        "    csv_filepath = os.path.join(folder_path_data, csv_filename)\n",
        "\n",
        "    names_files_list.append(csv_filename)\n",
        "    #------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Guardando el archivo descargado, en el repositorio del proyecto\n",
        "    with open(csv_filepath, 'wb') as csv_file:\n",
        "      csv_file.write(response_csv.content)\n",
        "\n",
        "    # Leer el archivo CSV con la codificación original\n",
        "    df = pd.read_csv(csv_filepath, encoding='latin1')\n",
        "\n",
        "    # Guardar el archivo CSV con codificación UTF-8\n",
        "    df.to_csv(csv_filepath, index=False, encoding='utf-8')\n",
        "  \n",
        "  except ValueError as ve:\n",
        "    # Manejo de errores para entradas no válidas (por ejemplo, si no se ingresa un número)\n",
        "    log_error_write(f\"Error de valor: {ve}\", 'download_csv_from_http')\n",
        "    print(f\"Error de valor: {ve}\")\n",
        "\n",
        "  except Exception as e:\n",
        "    # Manejo de errores para cualquier otro tipo de excepción no anticipada\n",
        "    log_error_write(f\"Ocurrió un error inesperado: {e}\", 'download_csv_from_http')\n",
        "    print(f\"Ocurrió un error inesperado: {e}\")\n",
        "\n",
        "  else:\n",
        "    # Este bloque se ejecuta si no se produce ninguna excepción en el bloque try\n",
        "    print(\"¡Proceso download_csv_from_http de descarga completado!\")\n",
        "\n",
        "  finally:\n",
        "    # Este bloque siempre se ejecuta, independientemente de si se produjo una excepción o no\n",
        "    print(\"¡Fin de la descarga!\")\n",
        "\n",
        "# *****************************************************************************************************************************************\n",
        "\n",
        "# Función utilizada para la descarga de archivos almacenados en google drive por parte de la institución donde se requiere obtener los datos.\n",
        "# - document: recibe el id del documento en google drive.\n",
        "# - description: Texto de la etiqueta de enlace, el cual describe la naturaleza de la información a descargar.\n",
        "# (Para descarga de documentos muy grandes, libreria gdown)\n",
        "def download_csv_from_http_high(document, description):\n",
        "\n",
        "  print(f\"link \" + description)\n",
        "\n",
        "  try:\n",
        "    # Verificando si la carpeta existe, si no, crearla\n",
        "    if not os.path.exists(folder_path_data):\n",
        "      os.makedirs(folder_path_data)\n",
        "\n",
        "    # Url de descarga a traves de google drive\n",
        "    csv_url = f'https://drive.google.com/uc?id={document}'\n",
        "\n",
        "    # Parte donde se hace una limpieza del parámetro description enviado,\n",
        "    # esto para construir el nombre que tendrá el archivo csv que se descarga a partir de los datos de las fuentes\n",
        "    # Utilizando expresión regular para reemplazar caracteres especiales y espacios por un solo guion bajo aplicándolo en la descripción del enlace.\n",
        "    csv_filename = re.sub(r'[^\\w\\s]+', '', description)\n",
        "    csv_filename = re.sub(r'\\s+', '', csv_filename)\n",
        "    csv_filename = unidecode(csv_filename)\n",
        "    csv_filename = csv_filename + \".csv\"\n",
        "\n",
        "    # Construyendo la ruta completa del archivo\n",
        "    csv_filepath = os.path.join(folder_path_data, csv_filename)\n",
        "\n",
        "    names_files_list.append(csv_filename)\n",
        "    #----------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Utilizando wget para descargar archivos grandes sin restricciones\n",
        "    gdown.download(csv_url, csv_filepath, quiet=False)\n",
        "\n",
        "    # Leer el archivo CSV con la codificación original\n",
        "    df = pd.read_csv(csv_filepath, encoding='latin1')\n",
        "\n",
        "    # Guardar el archivo CSV con codificación UTF-8\n",
        "    df.to_csv(csv_filepath, index=False, encoding='utf-8')\n",
        "  \n",
        "  except ValueError as ve:\n",
        "    # Manejo de errores para entradas no válidas (por ejemplo, si no se ingresa un número)\n",
        "    log_error_write(f\"Error de valor: {ve}\", 'download_csv_from_http_high')\n",
        "    print(f\"Error de valor: {ve}\")\n",
        "\n",
        "  except Exception as e:\n",
        "    # Manejo de errores para cualquier otro tipo de excepción no anticipada\n",
        "    log_error_write(f\"Ocurrió un error inesperado: {e}\", 'download_csv_from_http_high')\n",
        "    print(f\"Ocurrió un error inesperado: {e}\")\n",
        "\n",
        "  else:\n",
        "    # Este bloque se ejecuta si no se produce ninguna excepción en el bloque try\n",
        "    print(\"¡Proceso download_csv_from_http_high de descarga completado!\")\n",
        "\n",
        "  finally:\n",
        "    # Este bloque siempre se ejecuta, independientemente de si se produjo una excepción o no\n",
        "    print(\"¡Fin de la descarga!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Web Scraping , proceso de descarga de los datos de incidencia delicitva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gsk95W9Y_U6",
        "outputId": "dc72757f-acc8-4e01-f291-c331f9a0ae2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Descargando archivos proporcionados por https://www.gob.mx/sesnsp/acciones-y-programas/datos-abiertos-de-incidencia-delictiva\n",
            "link Cifras de Incidencia Delictiva Estatal, 2015 - agosto  2023\n",
            "¡Proceso download_csv_from_http de descarga completado!\n",
            "¡Fin de la descarga!\n",
            "link Cifras de Incidencia Delictiva Municipal, 2015 - agosto 2023. \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=13TjyJ9RkR49o0eWTFvhNqazeL4maORYp\n",
            "From (redirected): https://drive.google.com/uc?id=13TjyJ9RkR49o0eWTFvhNqazeL4maORYp&confirm=t&uuid=a4014816-bf75-4e3d-9941-55d483aa3125\n",
            "To: c:\\Proyectos\\Maestria\\IngenieriaCaracteristicas\\Proyectos\\IC-Proyecto-Indicadores-Delictivos\\rawdata\\CifrasdeIncidenciaDelictivaMunicipal2015agosto2023.csv\n",
            "100%|██████████| 306M/306M [00:41<00:00, 7.36MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¡Proceso download_csv_from_http_high de descarga completado!\n",
            "¡Fin de la descarga!\n",
            "Cifras de Incidencia Delictiva Municipal, 2015 - agosto 2023. : https://drive.google.com/file/d/13TjyJ9RkR49o0eWTFvhNqazeL4maORYp/view?usp=sharing\n",
            "link Cifras de Víctimas del Fuero Común, 2015 - agosto 2023\n",
            "¡Proceso download_csv_from_http de descarga completado!\n",
            "¡Fin de la descarga!\n",
            "link Cifras de Incidencia Delictiva Federal, 2012 - agosto 2023\n",
            "¡Proceso download_csv_from_http de descarga completado!\n",
            "¡Fin de la descarga!\n"
          ]
        }
      ],
      "source": [
        "# Descargando archivos proporcionados por https://www.gob.mx/sesnsp/acciones-y-programas/datos-abiertos-de-incidencia-delictiva\n",
        "print('# Descargando archivos proporcionados por https://www.gob.mx/sesnsp/acciones-y-programas/datos-abiertos-de-incidencia-delictiva')\n",
        "# Realizando Web scraping a la página donde están las distintas fuentes de datos\n",
        "# Esto asegura que los datos puedan ser descargados de forma automática sin necesidad de estar descargado uno por uno directamente desde la página.\n",
        "url = 'https://www.gob.mx/sesnsp/acciones-y-programas/datos-abiertos-de-incidencia-delictiva'\n",
        "\n",
        "try:\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.text, 'html.parser')\n",
        "  pretty_soup = soup.prettify()\n",
        "\n",
        "  # Obteniendo el texto dentro de la etiqueta <title>\n",
        "  incidencia_source_title = soup.title.text.strip()\n",
        "\n",
        "  # Separando el texto por los pipes \"|\", esto porque el texto original viene con dicho carácter especial\n",
        "  title_parts = incidencia_source_title.split('|')\n",
        "\n",
        "  # Limpiando los espacios en blanco alrededor de cada parte del titulo encontrado\n",
        "  title_parts = [part.strip() for part in title_parts]\n",
        "\n",
        "  incidencia_title = title_parts[0] + \" - \" + title_parts[1]\n",
        "\n",
        "  # Buscando todas las etiquetas <h2> y extraer su texto, esto ayudara a poder extraer la descripción de la página, la explicación de la razón de la página.\n",
        "  h2_tags = soup.find_all('h2')\n",
        "\n",
        "  # Se recorren cada una de los parrafos H2 encontrados para luego extraer el texto en donde comience con \"En esta pagina...\".\n",
        "  for h2_tag in h2_tags:\n",
        "      text_inside_h2 = h2_tag.text.strip()\n",
        "\n",
        "      if(text_inside_h2.find('En esta página')!=-1) :\n",
        "        incidencia_description = text_inside_h2\n",
        "\n",
        "  # Buscando todas las etiquetas <ul>, esto ayudara más adelante a poder realizar las descargas, pues aquí se encontrará las url de cada enlace.\n",
        "  ul_tags = soup.find_all('ul')\n",
        "\n",
        "  for ul_tag in ul_tags:\n",
        "      # Buscando todas las etiquetas <li> dentro de cada <ul>\n",
        "      li_tags = ul_tag.find_all('li')\n",
        "\n",
        "      for li_tag in li_tags:\n",
        "          # Buscando todas las etiquetas <a> dentro de cada <li> con texto que comienza con \"Cifras\"\n",
        "          a_tags = li_tag.find_all('a', href=True, string=lambda x: x and x.startswith('Cifras'))\n",
        "\n",
        "          for a_tag in a_tags:\n",
        "              # Obteniendo el valor del atributo href, para extraer su url\n",
        "              href_value = a_tag['href']\n",
        "\n",
        "              # Utilizando expresión regular para extraer la parte antes de \"/view\" de cada url esto para conseguir el id del documento\n",
        "              match = re.search(r'/file/d/(.*?)/view', href_value)\n",
        "              # match = re.search(r'(.*?)/view', href_value)\n",
        "\n",
        "              # En caso de encontrar coicidencia, se procede a extraer el id del documento de descarga.\n",
        "              if match:\n",
        "                extracted_part = match.group(1)\n",
        "                if a_tag.text.find(str(datetime.now().year)) != -1:\n",
        "                  # Condicionante para poder detectar si la descarga es de datos referentes a municipios, esto por conocimiento, son descargas más pesadas y por lo cual se ocupa de librería especial.\n",
        "                  if a_tag.text.find('Municipal') != -1:\n",
        "                    download_csv_from_http_high(extracted_part, a_tag.text)\n",
        "\n",
        "                    # Imprimiendo el tag descriptivo y la url para mostrarla en pantalla\n",
        "                    print(f\"\"+ a_tag.text + \": \" + href_value)\n",
        "                  else:\n",
        "                    download_csv_from_http(extracted_part, a_tag.text)\n",
        "\n",
        "                  #Creando listado que será usado para el documento con la descripción de la descarga de las distintas fuentes, de incidencia delictiva\n",
        "                  sources_list.append(f\"- {a_tag.text}: {href_value}\")\n",
        "except ValueError as ve:\n",
        "  # Manejo de errores para entradas no válidas (por ejemplo, si no se ingresa un número)\n",
        "  log_error_write(f\"Error de valor: {ve}\", 'web scraping')\n",
        "  print(f\"Error de valor: {ve}\")\n",
        "\n",
        "except Exception as e:\n",
        "  # Manejo de errores para cualquier otro tipo de excepción no anticipada\n",
        "  log_error_write(f\"Ocurrió un error inesperado: {e}\", 'web scraping')\n",
        "  print(f\"Ocurrió un error inesperado: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sección donde se crea el dataset que contiene los nombres de los diferentes archivos descargados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "_--TzngMSZL7"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # Convertiendo la lista de nombres de archivos generados a DataFrame\n",
        "    df_names = pd.DataFrame(names_files_list)\n",
        "\n",
        "    df_names = df_names.rename(columns={0: \"nombre\"})\n",
        "\n",
        "    # Guardando el DataFrame como un archivo CSV\n",
        "    df_names.to_csv(f'{folder_path_data}/nombres_dataset.csv', index=False, encoding='utf-8')\n",
        "except ValueError as ve:\n",
        "  # Manejo de errores para entradas no válidas (por ejemplo, si no se ingresa un número)\n",
        "  log_error_write(f\"Error de valor: {ve}\", 'Creación de dataset de nombres')\n",
        "  print(f\"Error de valor: {ve}\")\n",
        "\n",
        "except Exception as e:\n",
        "  # Manejo de errores para cualquier otro tipo de excepción no anticipada\n",
        "  log_error_write(f\"Ocurrió un error inesperado: {e}\", 'Creación de dataset de nombres')\n",
        "  print(f\"Ocurrió un error inesperado: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sección que crea el archivo txt de la descripción de las fuentes descargadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxfL8ahdP8VM",
        "outputId": "cdd1f38d-b18c-40ff-d463-1e8ad17f7e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archivos de datos descargados y descripción en txt creada.\n"
          ]
        }
      ],
      "source": [
        "# Creando el archivo de texto con descripción general de las fuentes de descargar y parte del proceso ETL\n",
        "\n",
        "try:\n",
        "   \n",
        "  # Verificando si la carpeta existe, si no, crearla\n",
        "  if not os.path.exists(folder_path_log):\n",
        "    os.makedirs(folder_path_log)\n",
        "\n",
        "  # Uniendo elementos de la lista de las fuentes en donde se han descargado los datos (sources_list)\n",
        "  texto_list = \"\\n\".join(sources_list)\n",
        "\n",
        "  date_now = datetime.now()\n",
        "\n",
        "  # Variable que contiene el texto que tendrá el archivo txt que se generará y descargará.\n",
        "  description = f\"\"\"\n",
        "  Proyecto: contando una historia a través de los datos (análisis de la variabilidad del comportamiento delictivo\n",
        "  en relación con factores temporales y geográficos, y su correlación con el desarrollo social y urbano para la prevención del delito)\n",
        "\n",
        "  Equipo participante:\n",
        "  - Miguel Ernesto Medina León\n",
        "  - Luis Andrés Burruel Durán\n",
        "  - Mario Estrada Ferreira\n",
        "\n",
        "  Descripción de los datos descargados:\n",
        "  - Las fuentes fueron descargadas y donde se puede encontrar información adicional en el sitio web: {incidencia_delictiva_link}\n",
        "\n",
        "  Fuentes:\n",
        "  {texto_list}\n",
        "\n",
        "  Fechas de descarga:\n",
        "  - El proceso de descarga de los datos se realizó el: {date_now.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "  \"\"\"\n",
        "  # Se crea el archivo a partir del texto de arriba, el cual se guardará con el nombre descriptivo más la fecha día*mes*año, para identificar la descarga por día.\n",
        "  with open(f'{folder_path_log}/fuentes_descripcion{date_now.strftime(\"%d%m%y\")}.txt', 'w') as description_file:\n",
        "      description_file.write(description)\n",
        "\n",
        "  print(\"Archivos de datos descargados y descripción en txt creada.\")\n",
        "\n",
        "except ValueError as ve:\n",
        "  # Manejo de errores para entradas no válidas (por ejemplo, si no se ingresa un número)\n",
        "  log_error_write(f\"Error de valor: {ve}\", 'Creación de archivo de descripción general')\n",
        "  print(f\"Error de valor: {ve}\")\n",
        "\n",
        "except Exception as e:\n",
        "  # Manejo de errores para cualquier otro tipo de excepción no anticipada\n",
        "  log_error_write(f\"Ocurrió un error inesperado: {e}\", 'Creación de archivo de descripción general')\n",
        "  print(f\"Ocurrió un error inesperado: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sección para dar validación de la cantidad de archivos descargados referente a incidencia delectiva, el numero esperado a la fecha es de 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "yiiB4K8JUcDg",
        "outputId": "4c0b1b44-7349-4987-b7bc-2ef5c19f1592"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "\n",
        "    nombres_dataset = pd.read_csv(f'{folder_path_data}/nombres_dataset.csv')\n",
        "    df_nombres_dataset = pd.DataFrame(nombres_dataset)\n",
        "\n",
        "    if df_nombres_dataset.value_counts().sum() != 4:\n",
        "       raise Exception('Hubo cambios en la cantidad de dataset considerados')\n",
        "    \n",
        "except ValueError as ve:\n",
        "  # Manejo de errores para entradas no válidas (por ejemplo, si no se ingresa un número)\n",
        "  log_error_write(f\"Error de valor: {ve}\", 'Verificación archivos')\n",
        "  print(f\"Error de valor: {ve}\")\n",
        "\n",
        "except Exception as e:\n",
        "  # Manejo de errores para cualquier otro tipo de excepción no anticipada\n",
        "  log_error_write(f\"Ocurrió un error inesperado: {e}\", 'Verificación archivos')\n",
        "  print(f\"Ocurrió un error inesperado: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
